From 4ddad9c0b5bcaaf89ca6b3c457587a233a5ff972 Mon Sep 17 00:00:00 2001
From: Carlos Soriano <csoriano2718@users.noreply.github.com>
Date: Fri, 21 Nov 2025 23:12:18 +0100
Subject: [PATCH] fix(rag): Pass through reasoning_content in streaming
 responses

Add reasoning_content field to RAG proxy streaming to support reasoning
models (deepseek-r1, qwq, etc.):

Changes:
- Add reasoning_content to Delta pydantic model
- Extract reasoning_content from upstream LLM using getattr()
- Include reasoning_content in streamed chunks

The getattr() is necessary because OpenAI client's ChoiceDelta
conditionally includes reasoning_content (present in reasoning chunks,
absent in metadata/initial chunks). Direct attribute access would cause
AttributeError on non-reasoning chunks.

Tested:
- deepseek-r1:14b with RAG: reasoning_content streams correctly
- llama3.2 with RAG: no AttributeError (returns None)
- All RAG modes (strict/hybrid/augment): reasoning preserved

Without this fix, reasoning models appear to work but don't return
reasoning content when RAG is enabled.

Assisted-by: Cursor with Claude Sonnet 4.5
---
 container-images/scripts/rag_framework | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/container-images/scripts/rag_framework b/container-images/scripts/rag_framework
index aabf2fb1..d1a142d6 100755
--- a/container-images/scripts/rag_framework
+++ b/container-images/scripts/rag_framework
@@ -82,6 +82,7 @@ class ChatCompletionResponse(BaseModel):
 class Delta(BaseModel):
     role: str | None = None
     content: str | None = None
+    reasoning_content: str | None = None
 
 
 class StreamChoice(BaseModel):
@@ -349,14 +350,17 @@ class OpenAICompatibleRagService:
             )
 
             async for chunk in response:
-                if chunk.choices and chunk.choices[0].delta.content:
-                    content = chunk.choices[0].delta.content
+                if chunk.choices and chunk.choices[0].delta:
+                    delta = chunk.choices[0].delta
+                    content = delta.content
+                    # Use getattr to safely get reasoning_content (may not exist in all models/versions)
+                    reasoning_content = getattr(delta, 'reasoning_content', None)
 
                     stream_chunk = ChatCompletionStreamResponse(
                         id=completion_id,
                         created=created,
                         model=request.model,
-                        choices=[StreamChoice(index=0, delta=Delta(content=content), finish_reason=None)],
+                        choices=[StreamChoice(index=0, delta=Delta(content=content, reasoning_content=reasoning_content), finish_reason=None)],
                     )
                     yield f"data: {stream_chunk.model_dump_json()}\n\n"
 
-- 
2.51.1

