From 7c07c531a1bceef5b334f602e46afc05588107ec Mon Sep 17 00:00:00 2001
From: Carlos Soriano <csoriano2718@users.noreply.github.com>
Date: Fri, 21 Nov 2025 23:12:18 +0100
Subject: [PATCH] Pass through reasoning_content in RAG proxy streaming

The RAG proxy was not forwarding reasoning_content from the upstream
model server to clients, causing reasoning models (deepseek-r1, qwq,
etc.) to appear to work but without returning their reasoning process.

Changes:
- Add reasoning_content field to Delta pydantic model
- Use getattr() to safely extract reasoning_content from upstream
  LLM responses (attribute may not exist in all chunks)
- Forward reasoning_content in ChatCompletionStreamResponse

The getattr() approach is necessary because the OpenAI Python client's
ChoiceDelta object conditionally includes reasoning_content - present
in reasoning chunks, absent in metadata/initial chunks. Direct attribute
access would cause AttributeError on non-reasoning chunks.

Tested with deepseek-r1:14b and llama3.2 - reasoning content now
correctly passes through the RAG proxy to clients.

Assisted-by: Cursor with Claude Sonnet 4.5
---
 container-images/scripts/rag_framework | 10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/container-images/scripts/rag_framework b/container-images/scripts/rag_framework
index aabf2fb1..d1a142d6 100755
--- a/container-images/scripts/rag_framework
+++ b/container-images/scripts/rag_framework
@@ -82,6 +82,7 @@ class ChatCompletionResponse(BaseModel):
 class Delta(BaseModel):
     role: str | None = None
     content: str | None = None
+    reasoning_content: str | None = None
 
 
 class StreamChoice(BaseModel):
@@ -349,14 +350,17 @@ class OpenAICompatibleRagService:
             )
 
             async for chunk in response:
-                if chunk.choices and chunk.choices[0].delta.content:
-                    content = chunk.choices[0].delta.content
+                if chunk.choices and chunk.choices[0].delta:
+                    delta = chunk.choices[0].delta
+                    content = delta.content
+                    # Use getattr to safely get reasoning_content (may not exist in all models/versions)
+                    reasoning_content = getattr(delta, 'reasoning_content', None)
 
                     stream_chunk = ChatCompletionStreamResponse(
                         id=completion_id,
                         created=created,
                         model=request.model,
-                        choices=[StreamChoice(index=0, delta=Delta(content=content), finish_reason=None)],
+                        choices=[StreamChoice(index=0, delta=Delta(content=content, reasoning_content=reasoning_content), finish_reason=None)],
                     )
                     yield f"data: {stream_chunk.model_dump_json()}\n\n"
 
-- 
2.51.1

